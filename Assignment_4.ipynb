{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97ba2526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\91971\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91971\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Collecting selenium\n",
      "  Obtaining dependency information for selenium from https://files.pythonhosted.org/packages/dc/72/96b5afa16908f9abc7c24b70adfd3a46c9740eb728ddfeab28379e38eaf9/selenium-4.16.0-py3-none-any.whl.metadata\n",
      "  Downloading selenium-4.16.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\91971\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: urllib3[socks]<3,>=1.26 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from selenium) (1.26.16)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Obtaining dependency information for trio~=0.17 from https://files.pythonhosted.org/packages/3e/14/746bb2b403af4be680ca0ae240d62473c4ec3b836024c2e85f27856d7991/trio-0.23.2-py3-none-any.whl.metadata\n",
      "  Downloading trio-0.23.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Obtaining dependency information for trio-websocket~=0.9 from https://files.pythonhosted.org/packages/48/be/a9ae5f50cad5b6f85bd2574c2c923730098530096e170c1ce7452394d7aa/trio_websocket-0.11.1-py3-none-any.whl.metadata\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.4)\n",
      "Requirement already satisfied: attrs>=20.1.0 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (22.1.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\91971\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\91971\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Obtaining dependency information for outcome from https://files.pythonhosted.org/packages/55/8b/5ab7257531a5d830fc8000c476e63c935488d74609b50f9384a643ec0a62/outcome-1.3.0.post0-py2.py3-none-any.whl.metadata\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\91971\\anaconda3\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\91971\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "     ---------------------------- ----------- 41.0/58.3 kB 1.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 58.3/58.3 kB 1.0 MB/s eta 0:00:00\n",
      "Downloading selenium-4.16.0-py3-none-any.whl (10.0 MB)\n",
      "   ---------------------------------------- 0.0/10.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/10.0 MB 3.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.3/10.0 MB 5.0 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.6/10.0 MB 5.3 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.1/10.0 MB 6.2 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.6/10.0 MB 7.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.1/10.0 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 2.7/10.0 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 3.1/10.0 MB 8.6 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.6/10.0 MB 9.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.2/10.0 MB 9.3 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.8/10.0 MB 9.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 5.4/10.0 MB 9.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.0/10.0 MB 10.0 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 6.5/10.0 MB 10.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.0/10.0 MB 10.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.6/10.0 MB 10.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.1/10.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.7/10.0 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.2/10.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.8/10.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.0/10.0 MB 10.4 MB/s eta 0:00:00\n",
      "Downloading trio-0.23.2-py3-none-any.whl (461 kB)\n",
      "   ---------------------------------------- 0.0/461.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 461.6/461.6 kB 14.6 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Installing collected packages: sniffio, outcome, h11, wsproto, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "Successfully installed h11-0.14.0 outcome-1.3.0.post0 selenium-4.16.0 sniffio-1.3.0 trio-0.23.2 trio-websocket-0.11.1 wsproto-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4\n",
    "!pip install selenium beautifulsoup4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a645c8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     25\u001b[0m     columns \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m     rank \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     27\u001b[0m     name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     28\u001b[0m     artist \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#ANS:1)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table that contains the data\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Initialize lists to store data\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Iterate through each row in the table\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    columns = row.find_all('td')\n",
    "    rank = columns[0].text.strip()\n",
    "    name = columns[1].text.strip()\n",
    "    artist = columns[2].text.strip()\n",
    "    upload_date = columns[3].text.strip()\n",
    "    views = columns[4].text.strip()\n",
    "\n",
    "    # Append data to the lists\n",
    "    rank_list.append(rank)\n",
    "    name_list.append(name)\n",
    "    artist_list.append(artist)\n",
    "    upload_date_list.append(upload_date)\n",
    "    views_list.append(views)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"A) Rank: {rank_list[i]}\")\n",
    "    print(f\"B) Name: {name_list[i]}\")\n",
    "    print(f\"C) Artist: {artist_list[i]}\")\n",
    "    print(f\"D) Upload date: {upload_date_list[i]}\")\n",
    "    print(f\"E) Views: {views_list[i]}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0f9eaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Find the link to the international fixtures page\u001b[39;00m\n\u001b[0;32m     14\u001b[0m fixtures_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/international/fixtures\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 15\u001b[0m fixtures_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.bcci.tv\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfixtures_link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Send a GET request to the international fixtures URL\u001b[39;00m\n\u001b[0;32m     18\u001b[0m fixtures_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(fixtures_url)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#ANS:2)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.bcci.tv/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_link = soup.find('a', {'href': '/international/fixtures'})\n",
    "fixtures_url = f\"https://www.bcci.tv{fixtures_link['href']}\"\n",
    "\n",
    "# Send a GET request to the international fixtures URL\n",
    "fixtures_response = requests.get(fixtures_url)\n",
    "\n",
    "# Parse the HTML content of the fixtures page\n",
    "fixtures_soup = BeautifulSoup(fixtures_response.text, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "series_list = []\n",
    "place_list = []\n",
    "date_list = []\n",
    "time_list = []\n",
    "\n",
    "# Find the relevant elements in the fixtures page\n",
    "fixture_elements = fixtures_soup.find_all('li', {'class': 'event-list-item'})\n",
    "\n",
    "# Iterate through each fixture element\n",
    "for fixture in fixture_elements:\n",
    "    series = fixture.find('h2').text.strip()\n",
    "    place = fixture.find('p', {'class': 'fixture__additional-info'}).text.strip()\n",
    "    date = fixture.find('div', {'class': 'fixture__date'}).text.strip()\n",
    "    time = fixture.find('span', {'class': 'fixture__time'}).text.strip()\n",
    "\n",
    "    # Append data to the lists\n",
    "    series_list.append(series)\n",
    "    place_list.append(place)\n",
    "    date_list.append(date)\n",
    "    time_list.append(time)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(series_list)):\n",
    "    print(f\"A) Series: {series_list[i]}\")\n",
    "    print(f\"B) Place: {place_list[i]}\")\n",
    "    print(f\"C) Date: {date_list[i]}\")\n",
    "    print(f\"D) Time: {time_list[i]}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a929247",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Find the link to the economy page\u001b[39;00m\n\u001b[0;32m     14\u001b[0m economy_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/economy\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 15\u001b[0m economy_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp://statisticstimes.com\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meconomy_link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Send a GET request to the economy URL\u001b[39;00m\n\u001b[0;32m     18\u001b[0m economy_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(economy_url)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#ANS:3)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"http://statisticstimes.com/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the economy page\n",
    "economy_link = soup.find('a', {'href': '/economy'})\n",
    "economy_url = f\"http://statisticstimes.com{economy_link['href']}\"\n",
    "\n",
    "# Send a GET request to the economy URL\n",
    "economy_response = requests.get(economy_url)\n",
    "\n",
    "# Parse the HTML content of the economy page\n",
    "economy_soup = BeautifulSoup(economy_response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the State-wise GDP page\n",
    "state_gdp_link = economy_soup.find('a', {'href': '/economy/gdp-of-indian-states'})\n",
    "state_gdp_url = f\"http://statisticstimes.com{state_gdp_link['href']}\"\n",
    "\n",
    "# Send a GET request to the State-wise GDP URL\n",
    "state_gdp_response = requests.get(state_gdp_url)\n",
    "\n",
    "# Parse the HTML content of the State-wise GDP page\n",
    "state_gdp_soup = BeautifulSoup(state_gdp_response.text, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "rank_list = []\n",
    "state_list = []\n",
    "gsdp_1819_list = []\n",
    "gsdp_1920_list = []\n",
    "share_1819_list = []\n",
    "gdp_billion_list = []\n",
    "\n",
    "# Find the relevant elements in the State-wise GDP page\n",
    "table_rows = state_gdp_soup.find('table', {'id': 'table_id'}).find('tbody').find_all('tr')\n",
    "\n",
    "# Iterate through each table row\n",
    "for row in table_rows:\n",
    "    columns = row.find_all('td')\n",
    "    rank = columns[0].text.strip()\n",
    "    state = columns[1].text.strip()\n",
    "    gsdp_1819 = columns[2].text.strip()\n",
    "    gsdp_1920 = columns[3].text.strip()\n",
    "    share_1819 = columns[4].text.strip()\n",
    "    gdp_billion = columns[5].text.strip()\n",
    "\n",
    "    # Append data to the lists\n",
    "    rank_list.append(rank)\n",
    "    state_list.append(state)\n",
    "    gsdp_1819_list.append(gsdp_1819)\n",
    "    gsdp_1920_list.append(gsdp_1920)\n",
    "    share_1819_list.append(share_1819)\n",
    "    gdp_billion_list.append(gdp_billion)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(rank_list)):\n",
    "    print(f\"A) Rank: {rank_list[i]}\")\n",
    "    print(f\"B) State: {state_list[i]}\")\n",
    "    print(f\"C) GSDP(18-19): {gsdp_1819_list[i]}\")\n",
    "    print(f\"D) GSDP(19-20): {gsdp_1920_list[i]}\")\n",
    "    print(f\"E) Share(18-19): {share_1819_list[i]}\")\n",
    "    print(f\"F) GDP($ billion): {gdp_billion_list[i]}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d09ed3b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchDriverException",
     "evalue": "Message: Unable to locate or obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchDriverException\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m chrome_service \u001b[38;5;241m=\u001b[39m ChromeService(chrome_path)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Start the Chrome browser\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(service\u001b[38;5;241m=\u001b[39mchrome_service, options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Navigate to the Billboard homepage\u001b[39;00m\n\u001b[0;32m     25\u001b[0m driver\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.billboard.com/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\chrome\\webdriver.py:45\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[1;34m(self, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     42\u001b[0m service \u001b[38;5;241m=\u001b[39m service \u001b[38;5;28;01mif\u001b[39;00m service \u001b[38;5;28;01melse\u001b[39;00m Service()\n\u001b[0;32m     43\u001b[0m options \u001b[38;5;241m=\u001b[39m options \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;28;01melse\u001b[39;00m Options()\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     46\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mDesiredCapabilities\u001b[38;5;241m.\u001b[39mCHROME[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     47\u001b[0m     vendor_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoog\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     48\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m     49\u001b[0m     service\u001b[38;5;241m=\u001b[39mservice,\n\u001b[0;32m     50\u001b[0m     keep_alive\u001b[38;5;241m=\u001b[39mkeep_alive,\n\u001b[0;32m     51\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\chromium\\webdriver.py:49\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[1;34m(self, browser_name, vendor_prefix, options, service, keep_alive)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new WebDriver instance of the ChromiumDriver. Starts the\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mservice and then creates new WebDriver instance of ChromiumDriver.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03m - keep_alive - Whether to configure ChromiumRemoteConnection to use HTTP keep-alive.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice \u001b[38;5;241m=\u001b[39m service\n\u001b[1;32m---> 49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mpath \u001b[38;5;241m=\u001b[39m DriverFinder\u001b[38;5;241m.\u001b[39mget_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice, options)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mstart()\n\u001b[0;32m     52\u001b[0m executor \u001b[38;5;241m=\u001b[39m ChromiumRemoteConnection(\n\u001b[0;32m     53\u001b[0m     remote_server_addr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mservice\u001b[38;5;241m.\u001b[39mservice_url,\n\u001b[0;32m     54\u001b[0m     browser_name\u001b[38;5;241m=\u001b[39mbrowser_name,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     57\u001b[0m     ignore_proxy\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39m_ignore_local_proxy,\n\u001b[0;32m     58\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\selenium\\webdriver\\common\\driver_finder.py:44\u001b[0m, in \u001b[0;36mDriverFinder.get_path\u001b[1;34m(service, options)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoSuchDriverException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to locate or obtain driver for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;241m.\u001b[39mcapabilities[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrowserName\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[1;31mNoSuchDriverException\u001b[0m: Message: Unable to locate or obtain driver for chrome; For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors/driver_location\n"
     ]
    }
   ],
   "source": [
    "#ANS:5)\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up the Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')  # Run Chrome in headless mode (no GUI)\n",
    "\n",
    "# Provide the path to your ChromeDriver executable\n",
    "chrome_path = '/path/to/chromedriver'\n",
    "\n",
    "# Set up the Chrome service\n",
    "chrome_service = ChromeService(chrome_path)\n",
    "\n",
    "# Start the Chrome browser\n",
    "driver = webdriver.Chrome(service=chrome_service, options=chrome_options)\n",
    "\n",
    "# Navigate to the Billboard homepage\n",
    "driver.get(\"https://www.billboard.com/\")\n",
    "\n",
    "# Click on the \"Charts\" option\n",
    "charts_option = driver.find_element(By.XPATH, '//li[contains(text(), \"Charts\")]')\n",
    "charts_option.click()\n",
    "\n",
    "# Click on the \"Hot 100\" page link\n",
    "hot_100_link = driver.find_element(By.XPATH, '//a[contains(text(), \"Hot 100\")]')\n",
    "hot_100_link.click()\n",
    "\n",
    "# Wait for the page to load\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'div.chart-list-item')))\n",
    "\n",
    "# Get the HTML content of the page\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "song_names = []\n",
    "artist_names = []\n",
    "last_week_ranks = []\n",
    "peak_ranks = []\n",
    "weeks_on_board = []\n",
    "\n",
    "# Find the relevant elements on the Hot 100 page\n",
    "chart_items = soup.find_all('div', class_='chart-list-item')\n",
    "\n",
    "# Iterate through each chart item\n",
    "for item in chart_items:\n",
    "    song_name = item.find('span', class_='chart-list-item__title-text').text.strip()\n",
    "    artist_name = item.find('div', class_='chart-list-item__artist').text.strip()\n",
    "    last_week_rank = item.find('div', class_='chart-list-item__last-week').text.strip()\n",
    "    peak_rank = item.find('div', class_='chart-list-item__weeks-at-one').text.strip()\n",
    "    weeks_on_board = item.find('div', class_='chart-list-item__weeks-on-chart').text.strip()\n",
    "\n",
    "    # Append data to the lists\n",
    "    song_names.append(song_name)\n",
    "    artist_names.append(artist_name)\n",
    "    last_week_ranks.append(last_week_rank)\n",
    "    peak_ranks.append(peak_rank)\n",
    "    weeks_on_board.append(weeks_on_board)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(song_names)):\n",
    "    print(f\"A) Song name: {song_names[i]}\")\n",
    "    print(f\"B) Artist name: {artist_names[i]}\")\n",
    "    print(f\"C) Last week rank: {last_week_ranks[i]}\")\n",
    "    print(f\"D) Peak rank: {peak_ranks[i]}\")\n",
    "    print(f\"E) Weeks on board: {weeks_on_board[i]}\")\n",
    "    print(\"------------------------\")\n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aba349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A) Book name: Da Vinci Code,The\n",
      "B) Author name: Brown, Dan\n",
      "C) Volumes sold: 5,094,805\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Harry Potter and the Deathly Hallows\n",
      "B) Author name: Rowling, J.K.\n",
      "C) Volumes sold: 4,475,152\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: Children's Fiction\n",
      "------------------------\n",
      "A) Book name: Harry Potter and the Philosopher's Stone\n",
      "B) Author name: Rowling, J.K.\n",
      "C) Volumes sold: 4,200,654\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: Children's Fiction\n",
      "------------------------\n",
      "A) Book name: Harry Potter and the Order of the Phoenix\n",
      "B) Author name: Rowling, J.K.\n",
      "C) Volumes sold: 4,179,479\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: Children's Fiction\n",
      "------------------------\n",
      "A) Book name: Fifty Shades of Grey\n",
      "B) Author name: James, E. L.\n",
      "C) Volumes sold: 3,758,936\n",
      "D) Publisher: Random House\n",
      "E) Genre: Romance & Sagas\n",
      "------------------------\n",
      "A) Book name: Harry Potter and the Goblet of Fire\n",
      "B) Author name: Rowling, J.K.\n",
      "C) Volumes sold: 3,583,215\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: Children's Fiction\n",
      "------------------------\n",
      "A) Book name: Harry Potter and the Chamber of Secrets\n",
      "B) Author name: Rowling, J.K.\n",
      "C) Volumes sold: 3,484,047\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: Children's Fiction\n",
      "------------------------\n",
      "A) Book name: Harry Potter and the Prisoner of Azkaban\n",
      "B) Author name: Rowling, J.K.\n",
      "C) Volumes sold: 3,377,906\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: Children's Fiction\n",
      "------------------------\n",
      "A) Book name: Angels and Demons\n",
      "B) Author name: Brown, Dan\n",
      "C) Volumes sold: 3,193,946\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Harry Potter and the Half-blood Prince:Children's Edition\n",
      "B) Author name: Rowling, J.K.\n",
      "C) Volumes sold: 2,950,264\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: Children's Fiction\n",
      "------------------------\n",
      "A) Book name: Fifty Shades Darker\n",
      "B) Author name: James, E. L.\n",
      "C) Volumes sold: 2,479,784\n",
      "D) Publisher: Random House\n",
      "E) Genre: Romance & Sagas\n",
      "------------------------\n",
      "A) Book name: Twilight\n",
      "B) Author name: Meyer, Stephenie\n",
      "C) Volumes sold: 2,315,405\n",
      "D) Publisher: Little, Brown Book\n",
      "E) Genre: Young Adult Fiction\n",
      "------------------------\n",
      "A) Book name: Girl with the Dragon Tattoo,The:Millennium Trilogy\n",
      "B) Author name: Larsson, Stieg\n",
      "C) Volumes sold: 2,233,570\n",
      "D) Publisher: Quercus\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Fifty Shades Freed\n",
      "B) Author name: James, E. L.\n",
      "C) Volumes sold: 2,193,928\n",
      "D) Publisher: Random House\n",
      "E) Genre: Romance & Sagas\n",
      "------------------------\n",
      "A) Book name: Lost Symbol,The\n",
      "B) Author name: Brown, Dan\n",
      "C) Volumes sold: 2,183,031\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: New Moon\n",
      "B) Author name: Meyer, Stephenie\n",
      "C) Volumes sold: 2,152,737\n",
      "D) Publisher: Little, Brown Book\n",
      "E) Genre: Young Adult Fiction\n",
      "------------------------\n",
      "A) Book name: Deception Point\n",
      "B) Author name: Brown, Dan\n",
      "C) Volumes sold: 2,062,145\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Eclipse\n",
      "B) Author name: Meyer, Stephenie\n",
      "C) Volumes sold: 2,052,876\n",
      "D) Publisher: Little, Brown Book\n",
      "E) Genre: Young Adult Fiction\n",
      "------------------------\n",
      "A) Book name: Lovely Bones,The\n",
      "B) Author name: Sebold, Alice\n",
      "C) Volumes sold: 2,005,598\n",
      "D) Publisher: Pan Macmillan\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Curious Incident of the Dog in the Night-time,The\n",
      "B) Author name: Haddon, Mark\n",
      "C) Volumes sold: 1,979,552\n",
      "D) Publisher: Random House\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Digital Fortress\n",
      "B) Author name: Brown, Dan\n",
      "C) Volumes sold: 1,928,900\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Short History of Nearly Everything,A\n",
      "B) Author name: Bryson, Bill\n",
      "C) Volumes sold: 1,852,919\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Popular Science\n",
      "------------------------\n",
      "A) Book name: Girl Who Played with Fire,The:Millennium Trilogy\n",
      "B) Author name: Larsson, Stieg\n",
      "C) Volumes sold: 1,814,784\n",
      "D) Publisher: Quercus\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Breaking Dawn\n",
      "B) Author name: Meyer, Stephenie\n",
      "C) Volumes sold: 1,787,118\n",
      "D) Publisher: Little, Brown Book\n",
      "E) Genre: Young Adult Fiction\n",
      "------------------------\n",
      "A) Book name: Very Hungry Caterpillar,The:The Very Hungry Caterpillar\n",
      "B) Author name: Carle, Eric\n",
      "C) Volumes sold: 1,783,535\n",
      "D) Publisher: Penguin\n",
      "E) Genre: Picture Books\n",
      "------------------------\n",
      "A) Book name: Gruffalo,The\n",
      "B) Author name: Donaldson, Julia\n",
      "C) Volumes sold: 1,781,269\n",
      "D) Publisher: Pan Macmillan\n",
      "E) Genre: Picture Books\n",
      "------------------------\n",
      "A) Book name: Jamie's 30-Minute Meals\n",
      "B) Author name: Oliver, Jamie\n",
      "C) Volumes sold: 1,743,266\n",
      "D) Publisher: Penguin\n",
      "E) Genre: Food & Drink: General\n",
      "------------------------\n",
      "A) Book name: Kite Runner,The\n",
      "B) Author name: Hosseini, Khaled\n",
      "C) Volumes sold: 1,629,119\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: One Day\n",
      "B) Author name: Nicholls, David\n",
      "C) Volumes sold: 1,616,068\n",
      "D) Publisher: Hodder & Stoughton\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Thousand Splendid Suns,A\n",
      "B) Author name: Hosseini, Khaled\n",
      "C) Volumes sold: 1,583,992\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\n",
      "B) Author name: Larsson, Stieg\n",
      "C) Volumes sold: 1,555,135\n",
      "D) Publisher: Quercus\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Time Traveler's Wife,The\n",
      "B) Author name: Niffenegger, Audrey\n",
      "C) Volumes sold: 1,546,886\n",
      "D) Publisher: Random House\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Atonement\n",
      "B) Author name: McEwan, Ian\n",
      "C) Volumes sold: 1,539,428\n",
      "D) Publisher: Random House\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Bridget Jones's Diary:A Novel\n",
      "B) Author name: Fielding, Helen\n",
      "C) Volumes sold: 1,508,205\n",
      "D) Publisher: Pan Macmillan\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: World According to Clarkson,The\n",
      "B) Author name: Clarkson, Jeremy\n",
      "C) Volumes sold: 1,489,403\n",
      "D) Publisher: Penguin\n",
      "E) Genre: Humour: Collections & General\n",
      "------------------------\n",
      "A) Book name: Captain Corelli's Mandolin\n",
      "B) Author name: Bernieres, Louis de\n",
      "C) Volumes sold: 1,352,318\n",
      "D) Publisher: Random House\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Sound of Laughter,The\n",
      "B) Author name: Kay, Peter\n",
      "C) Volumes sold: 1,310,207\n",
      "D) Publisher: Random House\n",
      "E) Genre: Autobiography: General\n",
      "------------------------\n",
      "A) Book name: Life of Pi\n",
      "B) Author name: Martel, Yann\n",
      "C) Volumes sold: 1,310,176\n",
      "D) Publisher: Canongate\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Billy Connolly\n",
      "B) Author name: Stephenson, Pamela\n",
      "C) Volumes sold: 1,231,957\n",
      "D) Publisher: HarperCollins\n",
      "E) Genre: Biography: The Arts\n",
      "------------------------\n",
      "A) Book name: Child Called It,A\n",
      "B) Author name: Pelzer, Dave\n",
      "C) Volumes sold: 1,217,712\n",
      "D) Publisher: Orion\n",
      "E) Genre: Autobiography: General\n",
      "------------------------\n",
      "A) Book name: Gruffalo's Child,The\n",
      "B) Author name: Donaldson, Julia\n",
      "C) Volumes sold: 1,208,711\n",
      "D) Publisher: Pan Macmillan\n",
      "E) Genre: Picture Books\n",
      "------------------------\n",
      "A) Book name: Angela's Ashes:A Memoir of a Childhood\n",
      "B) Author name: McCourt, Frank\n",
      "C) Volumes sold: 1,204,058\n",
      "D) Publisher: HarperCollins\n",
      "E) Genre: Autobiography: General\n",
      "------------------------\n",
      "A) Book name: Birdsong\n",
      "B) Author name: Faulks, Sebastian\n",
      "C) Volumes sold: 1,184,967\n",
      "D) Publisher: Random House\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Northern Lights:His Dark Materials S.\n",
      "B) Author name: Pullman, Philip\n",
      "C) Volumes sold: 1,181,503\n",
      "D) Publisher: Scholastic Ltd.\n",
      "E) Genre: Young Adult Fiction\n",
      "------------------------\n",
      "A) Book name: Labyrinth\n",
      "B) Author name: Mosse, Kate\n",
      "C) Volumes sold: 1,181,093\n",
      "D) Publisher: Orion\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Harry Potter and the Half-blood Prince\n",
      "B) Author name: Rowling, J.K.\n",
      "C) Volumes sold: 1,153,181\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: Science Fiction & Fantasy\n",
      "------------------------\n",
      "A) Book name: Help,The\n",
      "B) Author name: Stockett, Kathryn\n",
      "C) Volumes sold: 1,132,336\n",
      "D) Publisher: Penguin\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Man and Boy\n",
      "B) Author name: Parsons, Tony\n",
      "C) Volumes sold: 1,130,802\n",
      "D) Publisher: HarperCollins\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Memoirs of a Geisha\n",
      "B) Author name: Golden, Arthur\n",
      "C) Volumes sold: 1,126,337\n",
      "D) Publisher: Random House\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\n",
      "B) Author name: McCall Smith, Alexander\n",
      "C) Volumes sold: 1,115,549\n",
      "D) Publisher: Little, Brown Book\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Island,The\n",
      "B) Author name: Hislop, Victoria\n",
      "C) Volumes sold: 1,108,328\n",
      "D) Publisher: Headline\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: PS, I Love You\n",
      "B) Author name: Ahern, Cecelia\n",
      "C) Volumes sold: 1,107,379\n",
      "D) Publisher: HarperCollins\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: You are What You Eat:The Plan That Will Change Your Life\n",
      "B) Author name: McKeith, Gillian\n",
      "C) Volumes sold: 1,104,403\n",
      "D) Publisher: Penguin\n",
      "E) Genre: Fitness & Diet\n",
      "------------------------\n",
      "A) Book name: Shadow of the Wind,The\n",
      "B) Author name: Zafon, Carlos Ruiz\n",
      "C) Volumes sold: 1,092,349\n",
      "D) Publisher: Orion\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Tales of Beedle the Bard,The\n",
      "B) Author name: Rowling, J.K.\n",
      "C) Volumes sold: 1,090,847\n",
      "D) Publisher: Bloomsbury\n",
      "E) Genre: Children's Fiction\n",
      "------------------------\n",
      "A) Book name: Broker,The\n",
      "B) Author name: Grisham, John\n",
      "C) Volumes sold: 1,087,262\n",
      "D) Publisher: Random House\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\n",
      "B) Author name: Atkins, Robert C.\n",
      "C) Volumes sold: 1,054,196\n",
      "D) Publisher: Random House\n",
      "E) Genre: Fitness & Diet\n",
      "------------------------\n",
      "A) Book name: Subtle Knife,The:His Dark Materials S.\n",
      "B) Author name: Pullman, Philip\n",
      "C) Volumes sold: 1,037,160\n",
      "D) Publisher: Scholastic Ltd.\n",
      "E) Genre: Young Adult Fiction\n",
      "------------------------\n",
      "A) Book name: Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation\n",
      "B) Author name: Truss, Lynne\n",
      "C) Volumes sold: 1,023,688\n",
      "D) Publisher: Profile Books Group\n",
      "E) Genre: Usage & Writing Guides\n",
      "------------------------\n",
      "A) Book name: Delia's How to Cook:(Bk.1)\n",
      "B) Author name: Smith, Delia\n",
      "C) Volumes sold: 1,015,956\n",
      "D) Publisher: Random House\n",
      "E) Genre: Food & Drink: General\n",
      "------------------------\n",
      "A) Book name: Chocolat\n",
      "B) Author name: Harris, Joanne\n",
      "C) Volumes sold: 1,009,873\n",
      "D) Publisher: Transworld\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Boy in the Striped Pyjamas,The\n",
      "B) Author name: Boyne, John\n",
      "C) Volumes sold: 1,004,414\n",
      "D) Publisher: Random House Childrens Books G\n",
      "E) Genre: Young Adult Fiction\n",
      "------------------------\n",
      "A) Book name: My Sister's Keeper\n",
      "B) Author name: Picoult, Jodi\n",
      "C) Volumes sold: 1,003,780\n",
      "D) Publisher: Hodder & Stoughton\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Amber Spyglass,The:His Dark Materials S.\n",
      "B) Author name: Pullman, Philip\n",
      "C) Volumes sold: 1,002,314\n",
      "D) Publisher: Scholastic Ltd.\n",
      "E) Genre: Young Adult Fiction\n",
      "------------------------\n",
      "A) Book name: To Kill a Mockingbird\n",
      "B) Author name: Lee, Harper\n",
      "C) Volumes sold: 998,213\n",
      "D) Publisher: Random House\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Men are from Mars, Women are from Venus:A Practical Guide for Improvin\n",
      "B) Author name: Gray, John\n",
      "C) Volumes sold: 992,846\n",
      "D) Publisher: HarperCollins\n",
      "E) Genre: Popular Culture & Media: General Interest\n",
      "------------------------\n",
      "A) Book name: Dear Fatty\n",
      "B) Author name: French, Dawn\n",
      "C) Volumes sold: 986,753\n",
      "D) Publisher: Random House\n",
      "E) Genre: Autobiography: The Arts\n",
      "------------------------\n",
      "A) Book name: Short History of Tractors in Ukrainian,A\n",
      "B) Author name: Lewycka, Marina\n",
      "C) Volumes sold: 986,115\n",
      "D) Publisher: Penguin\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Hannibal\n",
      "B) Author name: Harris, Thomas\n",
      "C) Volumes sold: 970,509\n",
      "D) Publisher: Random House\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Lord of the Rings,The\n",
      "B) Author name: Tolkien, J. R. R.\n",
      "C) Volumes sold: 967,466\n",
      "D) Publisher: HarperCollins\n",
      "E) Genre: Science Fiction & Fantasy\n",
      "------------------------\n",
      "A) Book name: Stupid White Men:...and Other Sorry Excuses for the State of the Natio\n",
      "B) Author name: Moore, Michael\n",
      "C) Volumes sold: 963,353\n",
      "D) Publisher: Penguin\n",
      "E) Genre: Current Affairs & Issues\n",
      "------------------------\n",
      "A) Book name: Interpretation of Murder,The\n",
      "B) Author name: Rubenfeld, Jed\n",
      "C) Volumes sold: 962,515\n",
      "D) Publisher: Headline\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Sharon Osbourne Extreme:My Autobiography\n",
      "B) Author name: Osbourne, Sharon\n",
      "C) Volumes sold: 959,496\n",
      "D) Publisher: Little, Brown Book\n",
      "E) Genre: Autobiography: The Arts\n",
      "------------------------\n",
      "A) Book name: Alchemist,The:A Fable About Following Your Dream\n",
      "B) Author name: Coelho, Paulo\n",
      "C) Volumes sold: 956,114\n",
      "D) Publisher: HarperCollins\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: At My Mother's Knee ...:and Other Low Joints\n",
      "B) Author name: O'Grady, Paul\n",
      "C) Volumes sold: 945,640\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Autobiography: The Arts\n",
      "------------------------\n",
      "A) Book name: Notes from a Small Island\n",
      "B) Author name: Bryson, Bill\n",
      "C) Volumes sold: 931,312\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Travel Writing\n",
      "------------------------\n",
      "A) Book name: Return of the Naked Chef,The\n",
      "B) Author name: Oliver, Jamie\n",
      "C) Volumes sold: 925,425\n",
      "D) Publisher: Penguin\n",
      "E) Genre: Food & Drink: General\n",
      "------------------------\n",
      "A) Book name: Bridget Jones: The Edge of Reason\n",
      "B) Author name: Fielding, Helen\n",
      "C) Volumes sold: 924,695\n",
      "D) Publisher: Pan Macmillan\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Jamie's Italy\n",
      "B) Author name: Oliver, Jamie\n",
      "C) Volumes sold: 906,968\n",
      "D) Publisher: Penguin\n",
      "E) Genre: National & Regional Cuisine\n",
      "------------------------\n",
      "A) Book name: I Can Make You Thin\n",
      "B) Author name: McKenna, Paul\n",
      "C) Volumes sold: 905,086\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Fitness & Diet\n",
      "------------------------\n",
      "A) Book name: Down Under\n",
      "B) Author name: Bryson, Bill\n",
      "C) Volumes sold: 890,847\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Travel Writing\n",
      "------------------------\n",
      "A) Book name: Summons,The\n",
      "B) Author name: Grisham, John\n",
      "C) Volumes sold: 869,671\n",
      "D) Publisher: Random House\n",
      "E) Genre: Crime, Thriller & Adventure\n",
      "------------------------\n",
      "A) Book name: Small Island\n",
      "B) Author name: Levy, Andrea\n",
      "C) Volumes sold: 869,659\n",
      "D) Publisher: Headline\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Nigella Express\n",
      "B) Author name: Lawson, Nigella\n",
      "C) Volumes sold: 862,602\n",
      "D) Publisher: Random House\n",
      "E) Genre: Food & Drink: General\n",
      "------------------------\n",
      "A) Book name: Brick Lane\n",
      "B) Author name: Ali, Monica\n",
      "C) Volumes sold: 856,540\n",
      "D) Publisher: Transworld\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Memory Keeper's Daughter,The\n",
      "B) Author name: Edwards, Kim\n",
      "C) Volumes sold: 845,858\n",
      "D) Publisher: Penguin\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Room on the Broom\n",
      "B) Author name: Donaldson, Julia\n",
      "C) Volumes sold: 842,535\n",
      "D) Publisher: Pan Macmillan\n",
      "E) Genre: Picture Books\n",
      "------------------------\n",
      "A) Book name: About a Boy\n",
      "B) Author name: Hornby, Nick\n",
      "C) Volumes sold: 828,215\n",
      "D) Publisher: Penguin\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: My Booky Wook\n",
      "B) Author name: Brand, Russell\n",
      "C) Volumes sold: 820,563\n",
      "D) Publisher: Hodder & Stoughton\n",
      "E) Genre: Autobiography: The Arts\n",
      "------------------------\n",
      "A) Book name: God Delusion,The\n",
      "B) Author name: Dawkins, Richard\n",
      "C) Volumes sold: 816,907\n",
      "D) Publisher: Transworld\n",
      "E) Genre: Popular Science\n",
      "------------------------\n",
      "A) Book name: \"Beano\" Annual,The\n",
      "B) Author name: 0\n",
      "C) Volumes sold: 816,585\n",
      "D) Publisher: D.C. Thomson\n",
      "E) Genre: Children's Annuals\n",
      "------------------------\n",
      "A) Book name: White Teeth\n",
      "B) Author name: Smith, Zadie\n",
      "C) Volumes sold: 815,586\n",
      "D) Publisher: Penguin\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: House at Riverton,The\n",
      "B) Author name: Morton, Kate\n",
      "C) Volumes sold: 814,370\n",
      "D) Publisher: Pan Macmillan\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Book Thief,The\n",
      "B) Author name: Zusak, Markus\n",
      "C) Volumes sold: 809,641\n",
      "D) Publisher: Transworld\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Nights of Rain and Stars\n",
      "B) Author name: Binchy, Maeve\n",
      "C) Volumes sold: 808,900\n",
      "D) Publisher: Orion\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Ghost,The\n",
      "B) Author name: Harris, Robert\n",
      "C) Volumes sold: 807,311\n",
      "D) Publisher: Random House\n",
      "E) Genre: General & Literary Fiction\n",
      "------------------------\n",
      "A) Book name: Happy Days with the Naked Chef\n",
      "B) Author name: Oliver, Jamie\n",
      "C) Volumes sold: 794,201\n",
      "D) Publisher: Penguin\n",
      "E) Genre: Food & Drink: General\n",
      "------------------------\n",
      "A) Book name: Hunger Games,The:Hunger Games Trilogy\n",
      "B) Author name: Collins, Suzanne\n",
      "C) Volumes sold: 792,187\n",
      "D) Publisher: Scholastic Ltd.\n",
      "E) Genre: Young Adult Fiction\n",
      "------------------------\n",
      "A) Book name: Lost Boy,The:A Foster Child's Search for the Love of a Family\n",
      "B) Author name: Pelzer, Dave\n",
      "C) Volumes sold: 791,507\n",
      "D) Publisher: Orion\n",
      "E) Genre: Biography: General\n",
      "------------------------\n",
      "A) Book name: Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\n",
      "B) Author name: Oliver, Jamie\n",
      "C) Volumes sold: 791,095\n",
      "D) Publisher: Penguin\n",
      "E) Genre: Food & Drink: General\n",
      "------------------------\n"
     ]
    }
   ],
   "source": [
    "#ANS:6)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "book_names = []\n",
    "author_names = []\n",
    "volumes_sold = []\n",
    "publishers = []\n",
    "genres = []\n",
    "\n",
    "# Find the table that contains the data\n",
    "table = soup.find('table', {'class': 'in-article sortable'})\n",
    "\n",
    "# Iterate through each row in the table\n",
    "for row in table.find('tbody').find_all('tr'):\n",
    "    columns = row.find_all('td')\n",
    "    book_name = columns[1].text.strip()\n",
    "    author_name = columns[2].text.strip()\n",
    "    volumes = columns[3].text.strip()\n",
    "    publisher = columns[4].text.strip()\n",
    "    genre = columns[5].text.strip()\n",
    "\n",
    "    # Append data to the lists\n",
    "    book_names.append(book_name)\n",
    "    author_names.append(author_name)\n",
    "    volumes_sold.append(volumes)\n",
    "    publishers.append(publisher)\n",
    "    genres.append(genre)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(book_names)):\n",
    "    print(f\"A) Book name: {book_names[i]}\")\n",
    "    print(f\"B) Author name: {author_names[i]}\")\n",
    "    print(f\"C) Volumes sold: {volumes_sold[i]}\")\n",
    "    print(f\"D) Publisher: {publishers[i]}\")\n",
    "    print(f\"E) Genre: {genres[i]}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec6e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANS:7)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "names = []\n",
    "year_spans = []\n",
    "genres = []\n",
    "runtimes = []\n",
    "ratings = []\n",
    "votes = []\n",
    "\n",
    "# Find the relevant elements on the IMDb page\n",
    "series_items = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "# Iterate through each TV series item\n",
    "for series_item in series_items:\n",
    "    name = series_item.find('h3', class_='lister-item-header').find('a').text.strip()\n",
    "    year_span = series_item.find('span', class_='lister-item-year').text.strip()\n",
    "    genre = series_item.find('span', class_='genre').text.strip()\n",
    "    runtime = series_item.find('span', class_='runtime').text.strip()\n",
    "    rating = series_item.find('span', class_='ipl-rating-star__rating').text.strip()\n",
    "    votes_count = series_item.find('span', attrs={'name': 'nv'}).text.strip()\n",
    "\n",
    "    # Append data to the lists\n",
    "    names.append(name)\n",
    "    year_spans.append(year_span)\n",
    "    genres.append(genre)\n",
    "    runtimes.append(runtime)\n",
    "    ratings.append(rating)\n",
    "    votes.append(votes_count)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(names)):\n",
    "    print(f\"A) Name: {names[i]}\")\n",
    "    print(f\"B) Year span: {year_spans[i]}\")\n",
    "    print(f\"C) Genre: {genres[i]}\")\n",
    "    print(f\"D) Run time: {runtimes[i]}\")\n",
    "    print(f\"E) Ratings: {ratings[i]}\")\n",
    "    print(f\"F) Votes: {votes[i]}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a98ccb12",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Find the link to the \"Show All Dataset\" page\u001b[39;00m\n\u001b[0;32m     15\u001b[0m show_all_link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/ml/datasets.php\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m---> 16\u001b[0m show_all_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://archive.ics.uci.edu\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshow_all_link[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Send a GET request to the \"Show All Dataset\" page\u001b[39;00m\n\u001b[0;32m     19\u001b[0m show_all_response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(show_all_url)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "#ANS:8)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL for the UCI Machine Learning Repository home page\n",
    "url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content of the page\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the \"Show All Dataset\" page\n",
    "show_all_link = soup.find('a', {'href': '/ml/datasets.php'})\n",
    "show_all_url = f\"https://archive.ics.uci.edu{show_all_link['href']}\"\n",
    "\n",
    "# Send a GET request to the \"Show All Dataset\" page\n",
    "show_all_response = requests.get(show_all_url)\n",
    "\n",
    "# Parse the HTML content of the \"Show All Dataset\" page\n",
    "show_all_soup = BeautifulSoup(show_all_response.text, 'html.parser')\n",
    "\n",
    "# Initialize lists to store data\n",
    "dataset_names = []\n",
    "data_types = []\n",
    "tasks = []\n",
    "attribute_types = []\n",
    "no_of_instances = []\n",
    "no_of_attributes = []\n",
    "years = []\n",
    "\n",
    "# Find the relevant elements on the \"Show All Dataset\" page\n",
    "dataset_rows = show_all_soup.find_all('tr', {'valign': 'top'})\n",
    "\n",
    "# Iterate through each dataset row\n",
    "for dataset_row in dataset_rows:\n",
    "    columns = dataset_row.find_all('td', {'valign': 'top'})\n",
    "    \n",
    "    # Extract data from columns\n",
    "    dataset_name = columns[0].text.strip()\n",
    "    data_type = columns[1].text.strip()\n",
    "    task = columns[2].text.strip()\n",
    "    attribute_type = columns[3].text.strip()\n",
    "    no_of_instances_val = columns[4].text.strip()\n",
    "    no_of_attributes_val = columns[5].text.strip()\n",
    "    year = columns[6].text.strip()\n",
    "\n",
    "    # Append data to the lists\n",
    "    dataset_names.append(dataset_name)\n",
    "    data_types.append(data_type)\n",
    "    tasks.append(task)\n",
    "    attribute_types.append(attribute_type)\n",
    "    no_of_instances.append(no_of_instances_val)\n",
    "    no_of_attributes.append(no_of_attributes_val)\n",
    "    years.append(year)\n",
    "\n",
    "# Print the scraped data\n",
    "for i in range(len(dataset_names)):\n",
    "    print(f\"A) Dataset name: {dataset_names[i]}\")\n",
    "    print(f\"B) Data type: {data_types[i]}\")\n",
    "    print(f\"C) Task: {tasks[i]}\")\n",
    "    print(f\"D) Attribute type: {attribute_types[i]}\")\n",
    "    print(f\"E) No of instances: {no_of_instances[i]}\")\n",
    "    print(f\"F) No of attributes: {no_of_attributes[i]}\")\n",
    "    print(f\"G) Year: {years[i]}\")\n",
    "    print(\"------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d135884",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
